{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "params = {\n",
    "    'axes.grid' : True,\n",
    "    \"grid.linestyle\": '--',\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": \"Times New Roman\",\n",
    "}\n",
    "\n",
    "sns.set_style(\"ticks\", params)\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_eng_lang_order = [\"de\", \"fr\", \"nl\", \"pt\", \"ru\", \"zh\"]\n",
    "lang_pairs = list(chain.from_iterable([[f\"{lang}-en\", f\"en-{lang}\"] for lang in non_eng_lang_order]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_root_path = Path(\"<path to pretrained 7B flores results>\")\n",
    "best_adapters_root_path = Path(\"<path to best adapters 7B flores results>\")\n",
    "finetune_root_path = Path(\"<path to best finetuned 7B flores results>\")\n",
    "\n",
    "def load_scores(scores_file: Path):\n",
    "    lines = scores_file.read_text().splitlines()\n",
    "    scores = {}\n",
    "    for line in lines:\n",
    "        key, value = line.split(\": \")\n",
    "        scores[key] = float(value)\n",
    "    return scores\n",
    "\n",
    "def load_lp(data_root: Path, lp: str, instructions, ckpt: str):\n",
    "    sys_scores_path = data_root / lp / ckpt / instructions / \"sys_scores.txt\"\n",
    "    scores = load_scores(sys_scores_path)\n",
    "    return {\"lp\": lp, \"Direction\": \"En-XX\" if lp.startswith(\"en\") else \"XX-En\", **scores}\n",
    "\n",
    "\n",
    "def load_results(data_root: Path, instructions, ckpt: str):\n",
    "    results = []\n",
    "    lps_dirs = [d for d in data_root.iterdir() if d.is_dir()]\n",
    "    for lp_dir in lps_dirs:\n",
    "        lp = lp_dir.name\n",
    "        results.append(load_lp(data_root, lp, instructions, ckpt))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "pretrained_results = load_results(pretrained_root_path, \"few_shot_instructions2\", \"0\")\n",
    "pretrained_results[\"Model\"] = \"Pretrained\"\n",
    "finetune_results = load_results(finetune_root_path, \"zero_shot_instructions\", \"240000\")\n",
    "finetune_results[\"Model\"] = \"Finetuned\"\n",
    "best_adapters_results = load_results(best_adapters_root_path, \"zero_shot_instructions\", \"20000\")\n",
    "best_adapters_results[\"Model\"] = \"LoRA\"\n",
    "\n",
    "results = pd.concat([pretrained_results, finetune_results, best_adapters_results])\n",
    "results.rename(columns={\"lp\": \"Language Pair\" }, inplace=True)\n",
    "results[\"COMET-22\"] = results[\"COMET-22\"] * 100\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_zh_results = results[~results[\"Language Pair\"].str.contains(\"zh\")]\n",
    "no_zh_lang_pairs = [lp for lp in lang_pairs if not \"zh\" in lp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 3.5))\n",
    "g = sns.barplot(\n",
    "    data=no_zh_results, x=\"Language Pair\", y=\"COMET-22\", hue=\"Model\",\n",
    "    order=no_zh_lang_pairs,\n",
    "    #order=[\"En-XX\", \"XX-En\"],\n",
    "    hue_order=[\"Pretrained\", \"Finetuned\", \"LoRA\"],\n",
    "    ax=ax,\n",
    ")\n",
    "g.legend().set_title(\"\")\n",
    "g.set_ylabel(\"COMET\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(80, 90)\n",
    "sns.move_legend(\n",
    "    g, \"lower center\",\n",
    "    bbox_to_anchor=(.5, -.5), ncol=3, title=None, frameon=True,\n",
    ")\n",
    "#plt.savefig(\"figures/adapter_vs_finetuning.pdf\", bbox_inches=\"tight\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lp_all_ckpts(data_root: Path, lp: str):\n",
    "    ckpt_dirs = [d for d in (data_root / lp).iterdir() if d.is_dir()]\n",
    "    lp_results = []\n",
    "    for ckpt_dir in ckpt_dirs:\n",
    "        ckpt = ckpt_dir.name\n",
    "        sys_scores_path = ckpt_dir / \"zero_shot_instructions\" / \"sys_scores.txt\"\n",
    "        scores = load_scores(sys_scores_path)\n",
    "        lp_results.append({\"lp\": lp, \"Direction\": \"En-XX\" if lp.startswith(\"en\") else \"XX-En\", \"Context\": \"Zero-Shot\", \"Step\": int(ckpt), **scores})\n",
    "        sys_scores_path = ckpt_dir / \"few_shot_instructions2\" / \"sys_scores.txt\"\n",
    "        scores = load_scores(sys_scores_path)\n",
    "        lp_results.append({\"lp\": lp, \"Direction\": \"En-XX\" if lp.startswith(\"en\") else \"XX-En\", \"Context\": \"Few-Shot\", \"Step\": int(ckpt), **scores})\n",
    "    return lp_results\n",
    "\n",
    "\n",
    "def load_results_all_ckpts(data_root: Path):\n",
    "    results = []\n",
    "    lps_dirs = [d for d in data_root.iterdir() if d.is_dir()]\n",
    "    for lp_dir in lps_dirs:\n",
    "        lp = lp_dir.name\n",
    "        results.extend(load_lp_all_ckpts(data_root, lp))\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_adapters_results = load_results_all_ckpts(best_adapters_root_path)\n",
    "zero_shot_pretrained_results = load_results(pretrained_root_path, \"zero_shot_instructions\", \"0\")\n",
    "zero_shot_pretrained_results[\"Step\"] = 0\n",
    "zero_shot_pretrained_results[\"Context\"] = \"Zero-Shot\"\n",
    "\n",
    "full_adapters_results = pd.concat([zero_shot_pretrained_results, full_adapters_results])\n",
    "batch_size = 8\n",
    "full_adapters_results[\"Sequences\"] = full_adapters_results[\"Step\"] * batch_size\n",
    "full_adapters_results[\"COMET-22\"] = full_adapters_results[\"COMET-22\"] * 100\n",
    "full_adapters_results = full_adapters_results[\n",
    "    (full_adapters_results[\"Context\"] == \"Zero-Shot\") &\n",
    "    (~full_adapters_results[\"lp\"].str.contains(\"zh\"))\n",
    "]\n",
    "full_adapters_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_pretrained_results = load_results(pretrained_root_path, \"few_shot_instructions2\", \"0\")\n",
    "few_shot_pretrained_results[\"COMET-22\"] = few_shot_pretrained_results[\"COMET-22\"] * 100\n",
    "grouped = few_shot_pretrained_results.drop(columns=[\"lp\"]).groupby(\"Direction\").mean()\n",
    "print(grouped)\n",
    "en_xx_baseline = grouped.loc[\"En-XX\"][\"COMET-22\"]\n",
    "xx_en_baseline = grouped.loc[\"XX-En\"][\"COMET-22\"]\n",
    "en_xx_baseline, xx_en_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette()\n",
    "_, ax = plt.subplots(figsize=(5, 3.5))\n",
    "full_adapters_results[\"Legend\"] = \"Finetuned \" + full_adapters_results[\"Direction\"]\n",
    "ax.axhline(en_xx_baseline, 0, 20000, linestyle=\"dashed\", color=palette[0], label=\"Pretrained En-XX\")\n",
    "ax.axhline(xx_en_baseline, 0, 20000, linestyle=\"dashed\", color=palette[1], label=\"Pretrained XX-En\")\n",
    "g = sns.lineplot(\n",
    "    data=full_adapters_results,\n",
    "    x=\"Sequences\", y=\"COMET-22\", hue=\"Legend\", style=\"Legend\", \n",
    "    hue_order=[\"Finetuned En-XX\", \"Finetuned XX-En\"], markers=[\"o\", \"^\"], dashes=False, markersize=7,\n",
    "    ax=ax,\n",
    ")\n",
    "palette = sns.color_palette()\n",
    "g.legend().set_title(\"\")\n",
    "g.set_ylabel(\"COMET\")\n",
    "sns.move_legend(g, \"lower right\")\n",
    "ticks = np.array([0, 1000, 5000, 10000, 15000, 20000]) * batch_size\n",
    "xticklabels = [f\"{t // 1000}\" for t in ticks]\n",
    "xticklabels[0] = \"0\"\n",
    "plt.xticks(ticks, xticklabels)\n",
    "plt.ylim(70, 90)\n",
    "plt.xlabel(\"Training Examples (in thousands)\")\n",
    "sns.move_legend(\n",
    "    g, \"lower center\",\n",
    "    bbox_to_anchor=(.5, -.5), ncol=2, title=None, frameon=True,\n",
    ")\n",
    "#plt.savefig(\"figures/number_of_instructions.pdf\", bbox_inches=\"tight\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
