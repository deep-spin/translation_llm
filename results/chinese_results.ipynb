{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "params = {\n",
    "    'axes.grid' : True,\n",
    "    \"grid.linestyle\": '--',\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": \"Times New Roman\",\n",
    "}\n",
    "\n",
    "sns.set_style(\"ticks\", params)\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = Path(\"<path to pretrained 7B flores results>\")\n",
    "traditional_finetune_model = Path(\"<path to best finetuned 7B model flores results>\")\n",
    "zero_shot_model = Path(\"<path to best zero-shot adapters 7B model flores results>\")\n",
    "balanced_few_shot_model = Path(\"<path to best balanced few-shot adapters 7B model flores results>\")\n",
    "unbalanced_few_shot_model = Path(\"<path to best unbalanced few-shot adapters 7B model flores results>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(scores_file: Path):\n",
    "    lines = scores_file.read_text().splitlines()\n",
    "    scores = {}\n",
    "    for line in lines:\n",
    "        key, value = line.split(\": \")\n",
    "        scores[key] = float(value)\n",
    "    return scores\n",
    "\n",
    "def load_lp(data_root: Path, lp: str):\n",
    "    ckpt_dirs = [d for d in (data_root / lp).iterdir() if d.is_dir()]\n",
    "    lp_results = []\n",
    "    for ckpt_dir in ckpt_dirs:\n",
    "        ckpt = ckpt_dir.name\n",
    "        sys_scores_path = ckpt_dir / \"zero_shot_instructions\" / \"sys_scores.txt\"\n",
    "        scores = load_scores(sys_scores_path)\n",
    "        lp_results.append({\"lp\": lp, \"Context\": \"Zero-shot\", \"Step\": int(ckpt), **scores})\n",
    "        sys_scores_path = ckpt_dir / \"few_shot_instructions2\" / \"sys_scores.txt\"\n",
    "        scores = load_scores(sys_scores_path)\n",
    "        lp_results.append({\"lp\": lp, \"Context\": \"Five-shot\", \"Step\": int(ckpt), **scores})\n",
    "    return lp_results\n",
    "\n",
    "\n",
    "def load_results(data_root: Path):\n",
    "    results = []\n",
    "    lps_dirs = [d for d in data_root.iterdir() if d.is_dir()]\n",
    "    for lp_dir in lps_dirs:\n",
    "        lp = lp_dir.name\n",
    "        results.extend(load_lp(data_root, lp))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "pretrained_results = load_results(pretrained_model)\n",
    "pretrained_results[\"Model\"] = \"Pretrained\"\n",
    "zero_shot_results = load_results(zero_shot_model)\n",
    "zero_shot_results[\"Model\"] = \"FT w/o few-shot\"\n",
    "balanced_few_shot_results = load_results(balanced_few_shot_model)\n",
    "balanced_few_shot_results[\"Model\"] = \"FT w/ few-shot\"\n",
    "unbalanced_few_shot_results = load_results(unbalanced_few_shot_model)\n",
    "unbalanced_few_shot_results[\"Model\"] = \"Finetuned w ICL (unbalanced)\"\n",
    "#results = pd.concat([zero_shot_results, balanced_few_shot_results, unbalanced_few_shot_results])\n",
    "results = pd.concat([zero_shot_results, balanced_few_shot_results])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_results = pretrained_results[pretrained_results[\"lp\"].str.contains(\"zh\")]\n",
    "pretrained_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[results[\"lp\"].str.contains(\"zh\")]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 3.5))\n",
    "g = sns.barplot(\n",
    "    data=results[results[\"lp\"].str.contains(\"zh\")],\n",
    "    x=\"Model\",\n",
    "    y=\"COMET-22\",\n",
    "    hue=\"Context\",\n",
    "    errorbar=None,\n",
    "    ax=ax,\n",
    ")\n",
    "g.set_xlabel(\"\")\n",
    "g.set_ylabel(\"COMET\")\n",
    "plt.ylim(0.74, 0.78)\n",
    "sns.move_legend(\n",
    "    g, \"lower center\",\n",
    "    bbox_to_anchor=(.5, -.3), ncol=2, title=None, frameon=True,\n",
    ")\n",
    "#plt.savefig(\"figures/chinese_results.pdf\", bbox_inches=\"tight\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_root_path = Path(\"<path to pretrained 7B flores results>\")\n",
    "best_adapters_root_path = Path(\"<path to best zero-shot adapters 7B model flores results>\")\n",
    "finetune_root_path = Path(\"<path to best finetuned 7B model flores results>\")\n",
    "\n",
    "def load_scores(scores_file: Path):\n",
    "    lines = scores_file.read_text().splitlines()\n",
    "    scores = {}\n",
    "    for line in lines:\n",
    "        key, value = line.split(\": \")\n",
    "        scores[key] = float(value)\n",
    "    return scores\n",
    "\n",
    "def load_lp(data_root: Path, lp: str, instructions, ckpt: str):\n",
    "    sys_scores_path = data_root / lp / ckpt / instructions / \"sys_scores.txt\"\n",
    "    scores = load_scores(sys_scores_path)\n",
    "    return {\"lp\": lp, \"Direction\": \"En-XX\" if lp.startswith(\"en\") else \"XX-En\", **scores}\n",
    "\n",
    "\n",
    "def load_results(data_root: Path, instructions, ckpt: str):\n",
    "    results = []\n",
    "    lps_dirs = [d for d in data_root.iterdir() if d.is_dir()]\n",
    "    for lp_dir in lps_dirs:\n",
    "        lp = lp_dir.name\n",
    "        results.append(load_lp(data_root, lp, instructions, ckpt))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "pretrained_results = load_results(pretrained_root_path, \"few_shot_instructions2\", \"0\")\n",
    "pretrained_results[\"Model\"] = \"Pretrained\"\n",
    "finetune_results = load_results(finetune_root_path, \"zero_shot_instructions\", \"240000\")\n",
    "finetune_results[\"Model\"] = \"Finetuned\"\n",
    "best_adapters_results = load_results(best_adapters_root_path, \"zero_shot_instructions\", \"20000\")\n",
    "best_adapters_results[\"Model\"] = \"LoRA\"\n",
    "\n",
    "results = pd.concat([pretrained_results, finetune_results, best_adapters_results])\n",
    "results.rename(columns={\"lp\": \"Language Pair\" }, inplace=True)\n",
    "results[\"COMET-22\"] = results[\"COMET-22\"] * 100\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_zh_results = results[results[\"Language Pair\"].str.contains(\"zh\")]\n",
    "no_zh_lang_pairs = [\"zh-en\", \"en-zh\"]\n",
    "_, ax = plt.subplots(figsize=(5, 3.5))\n",
    "g = sns.barplot(\n",
    "    data=no_zh_results, x=\"Language Pair\", y=\"COMET-22\", hue=\"Model\",\n",
    "    order=no_zh_lang_pairs,\n",
    "    #order=[\"En-XX\", \"XX-En\"],\n",
    "    hue_order=[\"Pretrained\", \"Finetuned\", \"LoRA\"],\n",
    "    ax=ax,\n",
    ")\n",
    "g.legend().set_title(\"\")\n",
    "g.set_ylabel(\"COMET\")\n",
    "plt.ylim(60, 85)\n",
    "sns.move_legend(\n",
    "    g, \"lower center\",\n",
    "    bbox_to_anchor=(.5, -.4), ncol=3, title=None, frameon=True,\n",
    ")\n",
    "#plt.savefig(\"figures/chinese_adapter_vs_finetuning.pdf\", bbox_inches=\"tight\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
